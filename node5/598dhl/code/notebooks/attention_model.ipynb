{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVnGSv6cxVKj"
      },
      "outputs": [],
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import math\n",
        "\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Transformer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import BertTokenizerFast, BertModel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaoYuRw0o0W8"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import download_url, Data\n",
        "from torch_geometric.data import Dataset as GeoDataset\n",
        "from torch_geometric.data import DataLoader as GeoDataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gpao3ymyTBg"
      },
      "outputs": [],
      "source": [
        "#Need a special generator for random sampling:\n",
        "\n",
        "\n",
        "class GenerateData():\n",
        "  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
        "    self.path_train = path_train\n",
        "    self.path_val = path_val\n",
        "    self.path_test = path_test\n",
        "    self.path_molecules = path_molecules\n",
        "    self.path_token_embs = path_token_embs\n",
        "\n",
        "    self.mol_trunc_length = 512\n",
        "    self.text_trunc_length = 256 \n",
        "\n",
        "    self.prep_text_tokenizer()\n",
        "    \n",
        "    self.load_substructures()\n",
        "\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.store_descriptions()\n",
        "    \n",
        "  def load_substructures(self):\n",
        "    self.molecule_sentences = {}\n",
        "    self.molecule_tokens = {}\n",
        "\n",
        "    total_tokens = set()\n",
        "    self.max_mol_length = 0\n",
        "    with open(self.path_molecules) as f:\n",
        "      for line in f:\n",
        "        spl = line.split(\":\")\n",
        "        cid = spl[0]\n",
        "        tokens = spl[1].strip()\n",
        "        self.molecule_sentences[cid] = tokens\n",
        "        t = tokens.split()\n",
        "        total_tokens.update(t)\n",
        "        size = len(t)\n",
        "        if size > self.max_mol_length: self.max_mol_length = size\n",
        "\n",
        "\n",
        "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
        "\n",
        "\n",
        "\n",
        "  def prep_text_tokenizer(self):\n",
        "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        " \n",
        "\n",
        "  def store_descriptions(self):\n",
        "    self.descriptions = {}\n",
        "    \n",
        "    self.mols = {}\n",
        "\n",
        "\n",
        "\n",
        "    self.training_cids = []\n",
        "    #get training set cids...\n",
        "    with open(self.path_train) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.training_cids.append(line['cid'])\n",
        "        \n",
        "    self.validation_cids = []\n",
        "    #get validation set cids...\n",
        "    with open(self.path_val) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.validation_cids.append(line['cid'])\n",
        "        \n",
        "    self.test_cids = []\n",
        "    with open(self.path_test) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.test_cids.append(line['cid'])\n",
        "\n",
        "  #transformers can't take array with full attention so have to pad a 0...\n",
        "  def padarray(self, A, size, value=0):\n",
        "      t = size - len(A)\n",
        "      return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
        "\n",
        "\n",
        "  def generate_examples_train(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.training_cids)\n",
        "\n",
        "    for cid in self.training_cids:\n",
        "      label = np.random.randint(2)\n",
        "      rand_cid = np.random.choice(self.training_cids)\n",
        "      if label:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "      else:\n",
        "        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "\n",
        "      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
        "      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_ids,\n",
        "                'attention_mask': text_mask,\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },         \n",
        "          },\n",
        "          'label': label\n",
        "      }\n",
        "\n",
        "\n",
        "  def generate_examples_val(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.validation_cids)\n",
        "\n",
        "    for cid in self.validation_cids:\n",
        "      label = np.random.randint(2)\n",
        "      rand_cid = np.random.choice(self.validation_cids)\n",
        "      if label:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "      else:\n",
        "        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "\n",
        "\n",
        "      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
        "      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_ids,\n",
        "                'attention_mask': text_mask,\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },         \n",
        "          },\n",
        "          'label': label\n",
        "      }\n",
        "\n",
        "  def generate_examples_test(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.test_cids)\n",
        "\n",
        "    for cid in self.test_cids:\n",
        "      label = np.random.randint(2)\n",
        "      rand_cid = np.random.choice(self.test_cids)\n",
        "      if label:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "      else:\n",
        "        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "\n",
        "\n",
        "      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
        "      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_ids,\n",
        "                'attention_mask': text_mask,\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },         \n",
        "          },\n",
        "          'label': label\n",
        "      }\n",
        "\n",
        "\n",
        "\n",
        "mounted_path_token_embs = \"input/token_embedding_dict.npy\"\n",
        "mounted_path_train = \"input/mol2vec_ChEBI_20_training.txt\"\n",
        "mounted_path_val = \"input/mol2vec_ChEBI_20_val.txt\"\n",
        "mounted_path_test = \"input/mol2vec_ChEBI_20_test.txt\"\n",
        "mounted_path_molecules = \"input/ChEBI_defintions_substructure_corpus.cp\"\n",
        "gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zck-zGTa8JOv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, gen, length):\n",
        "      'Initialization'\n",
        "\n",
        "      self.gen = gen\n",
        "      self.it = iter(self.gen())\n",
        "\n",
        "      self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "      'Denotes the total number of samples'\n",
        "      return self.length\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      'Generates one sample of data'\n",
        "\n",
        "      try:\n",
        "        ex = next(self.it)\n",
        "      except StopIteration:\n",
        "        self.it = iter(self.gen())\n",
        "        ex = next(self.it)\n",
        "\n",
        "      X = ex['input']\n",
        "      y = ex['label']\n",
        "\n",
        "      return X, y\n",
        "\n",
        "training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n",
        "validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n",
        "test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fj8h8vhk3W0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Parameters\n",
        "params = {'batch_size': gt.batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "training_generator = DataLoader(training_set, **params)\n",
        "validation_generator = DataLoader(validation_set, **params)\n",
        "test_generator = DataLoader(test_set, **params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MVm3jz5pIcf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MoleculeGraphDataset(GeoDataset):\n",
        "    def __init__(self, root, cids, data_path, gt, transform=None, pre_transform=None):\n",
        "        self.cids = cids\n",
        "        self.data_path = data_path\n",
        "        self.gt = gt\n",
        "        super(MoleculeGraphDataset, self).__init__(root, transform, pre_transform)\n",
        "        \n",
        "        self.idx_to_cid = {}\n",
        "        i = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            cid = int(raw_path.split('/')[-1][:-6])\n",
        "            self.idx_to_cid[i] = cid\n",
        "            i += 1\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [cid + \".graph\" for cid in self.cids]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data_{}.pt'.format(cid) for cid in self.cids]\n",
        "\n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        shutil.copy(self.data_path, os.path.join(self.raw_dir, \"/mol_graphs.zip\"))\n",
        "        \n",
        "    def process_graph(self, raw_path):\n",
        "      edge_index  = []\n",
        "      x = []\n",
        "      with open(raw_path, 'r') as f:\n",
        "        next(f)\n",
        "        for line in f: #edges\n",
        "          if line != \"\\n\":\n",
        "            edge = *map(int, line.split()), \n",
        "            edge_index.append(edge)\n",
        "          else:\n",
        "            break\n",
        "        next(f)\n",
        "        for line in f: #get mol2vec features:\n",
        "          substruct_id = line.strip().split()[-1]\n",
        "          if substruct_id in self.gt.token_embs:\n",
        "            x.append(self.gt.token_embs[substruct_id])\n",
        "          else:\n",
        "            x.append(self.gt.token_embs['UNK'])\n",
        "\n",
        "        return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self):\n",
        "      \n",
        "        with zipfile.ZipFile(os.path.join(self.raw_dir, \"/mol_graphs.zip\"), 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.raw_dir)\n",
        "\n",
        "\n",
        "        i = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            # Read data from `raw_path`.\n",
        "\n",
        "            cid = int(raw_path.split('/')[-1][:-6])\n",
        "\n",
        "            edge_index, x = self.process_graph(raw_path)\n",
        "            data = Data(x=x, edge_index = edge_index)\n",
        "\n",
        "            if self.pre_filter is not None and not self.pre_filter(data):\n",
        "                continue\n",
        "\n",
        "            if self.pre_transform is not None:\n",
        "                data = self.pre_transform(data)\n",
        "\n",
        "            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
        "            i += 1\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(self.idx_to_cid[idx])))\n",
        "        return data\n",
        "\n",
        "    def get_cid(self, cid):\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
        "        return data\n",
        "\n",
        "#To get specific lists...\n",
        "\n",
        "class CustomGraphCollater(object):\n",
        "    def __init__(self, dataset, mask_len, follow_batch = [], exclude_keys = []):\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "        self.dataset = dataset\n",
        "        self.mask_len = mask_len\n",
        "        self.mask_indices = np.array(range(mask_len))\n",
        "\n",
        "    def generate_mask(self, sz):\n",
        "        rv = torch.zeros((self.mask_len), dtype = torch.bool)\n",
        "        rv = rv.masked_fill(torch.BoolTensor(self.mask_indices < sz), bool(1)) #pytorch transformer input version\n",
        "        rv[-1] = 0 #set last value to 0 because pytorch can't handle all 1s\n",
        "        return rv\n",
        "\n",
        "    def get_masks(self, batch):\n",
        "      return torch.stack([self.generate_mask(b.x.shape[0]) for b in batch])\n",
        "\n",
        "    def collate(self, batch):\n",
        "        elem = batch[0]\n",
        "        if isinstance(elem, Data):\n",
        "            return Batch.from_data_list(batch) \n",
        "            \n",
        "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
        "\n",
        "    def __call__(self, cids):\n",
        "      \n",
        "        tmp = [self.dataset.get_cid(int(cid)) for cid in cids]\n",
        "        return self.collate(tmp), self.get_masks(tmp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkfbSCMipLbW"
      },
      "outputs": [],
      "source": [
        "root = 'graph-data/'\n",
        "graph_data_path = \"input/mol_graphs.zip\"\n",
        "\n",
        "\n",
        "mg_data_tr = MoleculeGraphDataset(root, gt.training_cids, graph_data_path, gt)\n",
        "graph_batcher_tr = CustomGraphCollater(mg_data_tr, gt.mol_trunc_length)\n",
        "\n",
        "mg_data_val = MoleculeGraphDataset(root, gt.validation_cids, graph_data_path, gt)\n",
        "graph_batcher_val = CustomGraphCollater(mg_data_val, gt.mol_trunc_length)\n",
        "\n",
        "mg_data_test = MoleculeGraphDataset(root, gt.test_cids, graph_data_path, gt)\n",
        "graph_batcher_test = CustomGraphCollater(mg_data_test, gt.mol_trunc_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aksj743St9ga"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, nhead, nlayers, graph_hidden_channels, mol_trunc_length,  dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        self.text_hidden1 = nn.Linear(ninp, nhid)\n",
        "        self.text_hidden2 = nn.Linear(nhid, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "        self.graph_hidden_channels = graph_hidden_channels\n",
        "\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.text_transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
        "        \n",
        "\n",
        "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "        self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm((nout))\n",
        "        self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "        \n",
        "        #For GCN:\n",
        "        self.conv1 = GCNConv(mg_data_val.num_node_features, graph_hidden_channels)\n",
        "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
        "\n",
        "\n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "        \n",
        "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "        self.device = 'cpu' \n",
        "\n",
        "    def set_device(self, dev):\n",
        "        self.to(dev)\n",
        "        self.device = dev\n",
        "\n",
        "    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n",
        "      \n",
        "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
        "\n",
        "        #Obtain node embeddings \n",
        "        x = graph_batch.x\n",
        "        edge_index = graph_batch.edge_index\n",
        "        batch = graph_batch.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        mol_x = self.conv3(x, edge_index)\n",
        "\n",
        "        #turn pytorch geometric output into the correct format for transformer\n",
        "        #requires recovering the nodes from each graph into a separate dimension\n",
        "        node_features = torch.zeros((graph_batch.num_graphs, gt.mol_trunc_length, self.graph_hidden_channels)).to(self.device)\n",
        "        for i, p in enumerate(graph_batch.ptr):\n",
        "          if p == 0: \n",
        "            old_p = p\n",
        "            continue\n",
        "          node_features[i - 1, :p-old_p, :] = mol_x[old_p:torch.min(p, old_p + gt.mol_trunc_length), :]\n",
        "          old_p = p\n",
        "        node_features = torch.transpose(node_features, 0, 1)\n",
        "\n",
        "        text_output = self.text_transformer_decoder(text_encoder_output['last_hidden_state'].transpose(0,1), node_features, \n",
        "                                                            tgt_key_padding_mask = text_mask == 0, memory_key_padding_mask = ~molecule_mask) \n",
        "\n",
        "\n",
        "        #Readout layer\n",
        "        x = global_mean_pool(mol_x, batch)  # [batch_size, graph_hidden_channels]\n",
        "\n",
        "        x = self.mol_hidden1(x)\n",
        "        x = x.relu()\n",
        "        x = self.mol_hidden2(x)\n",
        "\n",
        "        text_x = torch.tanh(self.text_hidden1(text_output[0,:,:])) #[CLS] pooler\n",
        "        text_x = self.text_hidden2(text_x)\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        text_x = self.ln2(text_x)\n",
        "\n",
        "        x = x * torch.exp(self.temp)\n",
        "        text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGMF8AZcB2Zy"
      },
      "outputs": [],
      "source": [
        "model = Model(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nout = 300, nhead = 8, nhid = 512, nlayers = 3, graph_hidden_channels = 768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 40\n",
        "init_lr = 1e-4 \n",
        "bert_lr = 3e-5\n",
        "bert_params = list(model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(training_generator) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1heECu1nVRB"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "tmp = model.set_device(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HytSaAyHNBuZ"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def loss_func(v1, v2, labels):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  eye = torch.diag_embed(labels).to(device)\n",
        "  return criterion(logits, eye) + criterion(torch.transpose(logits, 0, 1), eye), logits.diag() > 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtfDFAnN_Neu"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "mounted_path = \"outputs/\"\n",
        "if not os.path.exists(mounted_path):\n",
        "  os.mkdir(mounted_path)\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    \n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "    for i, d in enumerate(training_generator):\n",
        "        batch, labels = d\n",
        "        # Transfer to GPU\n",
        "\n",
        "        text = batch['text']['input_ids'].to(device)\n",
        "        text_mask = batch['text']['attention_mask'].bool().to(device)\n",
        "        graph_batch, molecule_mask = graph_batcher_tr(d[0]['molecule']['cid'])\n",
        "        graph_batch = graph_batch.to(device)\n",
        "        molecule_mask = molecule_mask.to(device)\n",
        "\n",
        "\n",
        "        labels = labels.float().to(device)\n",
        "        \n",
        "        text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "        loss, pred = loss_func(text_out, chem_out, labels)\n",
        "        if torch.isnan(loss): zz\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) == labels.cpu().detach().numpy()) / labels.shape[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \"Acc:\", str(running_acc / (i+1)), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "    train_losses.append(running_loss / (i+1))\n",
        "    train_acc.append(running_acc / (i+1))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "    print(\"Training accuracy:\", train_acc[-1])\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "      start_time = time.time()\n",
        "      running_acc = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, d in enumerate(validation_generator):\n",
        "          batch, labels = d\n",
        "          # Transfer to GPU\n",
        "        \n",
        "          text = batch['text']['input_ids'].to(device)\n",
        "          text_mask = batch['text']['attention_mask'].bool().to(device)\n",
        "          graph_batch, molecule_mask = graph_batcher_val(d[0]['molecule']['cid'])\n",
        "          graph_batch = graph_batch.to(device)\n",
        "          molecule_mask = molecule_mask.to(device)\n",
        "\n",
        "          labels = labels.float().to(device)\n",
        "          \n",
        "          text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "          loss, pred = loss_func(text_out, chem_out, labels)\n",
        "\n",
        "          running_loss += loss.item()\n",
        "          running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) == labels.cpu().detach().numpy()) / labels.shape[0]\n",
        "\n",
        "          \n",
        "          if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "      val_losses.append(running_loss / (i+1))\n",
        "      val_acc.append(running_acc / (i+1))\n",
        "\n",
        "\n",
        "      min_loss = np.min(val_losses)\n",
        "      if val_losses[-1] == min_loss:\n",
        "          torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n",
        "      \n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "    print(\"Validation accuracy:\", val_acc[-1])\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYHuY5GkbmfK"
      },
      "source": [
        "#Extract attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDmzJpf8boQS"
      },
      "outputs": [],
      "source": [
        "last_decoder = model.text_transformer_decoder.layers[-1]\n",
        "\n",
        "mha_weights = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        mha_weights[cid] = output[1].cpu().detach().numpy()\n",
        "    return hook\n",
        "\n",
        "\n",
        "handle = last_decoder.multihead_attn.register_forward_hook(get_activation(''))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVRbgPCZbsN1"
      },
      "outputs": [],
      "source": [
        "for i,d in enumerate(gt.generate_examples_train()):\n",
        "\n",
        "  batch = d['input']\n",
        "\n",
        "  cid = d['cid']#batch['molecule']['cid'][0]\n",
        "  text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "  text = torch.Tensor(batch['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "  graph_batch, molecule_mask = graph_batcher_val([batch['molecule']['cid']])\n",
        "  graph_batch = graph_batch.to(device)\n",
        "  molecule_mask = molecule_mask.to(device)\n",
        "  graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
        "    \n",
        "  out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "  \n",
        "  #for memory reasons\n",
        "  mol_length = graph_batch.x.shape[0]\n",
        "  text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                    max_length=gt.text_trunc_length - 1)\n",
        "  text_length = np.sum(text_input['attention_mask'])\n",
        "  \n",
        "  mha_weights[cid] = mha_weights[cid][0,:text_length, :mol_length]\n",
        "\n",
        "  if (i+1) % 1000 == 0: print(i+1)\n",
        "\n",
        "for i,d in enumerate(gt.generate_examples_val()):\n",
        "\n",
        "  batch = d['input']\n",
        "\n",
        "  cid = d['cid']#batch['molecule']['cid'][0]\n",
        "  text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "  text = torch.Tensor(batch['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "  graph_batch, molecule_mask = graph_batcher_val([batch['molecule']['cid']])\n",
        "  graph_batch = graph_batch.to(device)\n",
        "  molecule_mask = molecule_mask.to(device)\n",
        "  graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
        "    \n",
        "  \n",
        "  out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "\n",
        "  #for memory reasons\n",
        "  mol_length = graph_batch.x.shape[0]\n",
        "  text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                    max_length=gt.text_trunc_length - 1)\n",
        "  text_length = np.sum(text_input['attention_mask'])\n",
        "  mha_weights[cid] = mha_weights[cid][0,:text_length, :mol_length]\n",
        "\n",
        "  \n",
        "  if (i+1) % 1000 == 0: print(i+1)\n",
        "  \n",
        "for i,d in enumerate(gt.generate_examples_test()):\n",
        "\n",
        "  batch = d['input']\n",
        "\n",
        "  cid = d['cid']\n",
        "  text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "  text = torch.Tensor(batch['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "  graph_batch, molecule_mask = graph_batcher_test([batch['molecule']['cid']])\n",
        "  graph_batch = graph_batch.to(device)\n",
        "  molecule_mask = molecule_mask.to(device)\n",
        "  graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
        "    \n",
        "  \n",
        "  out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "\n",
        "  #for memory reasons\n",
        "  mol_length = graph_batch.x.shape[0]\n",
        "  text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                    max_length=gt.text_trunc_length - 1)\n",
        "  text_length = np.sum(text_input['attention_mask'])\n",
        "  mha_weights[cid] = mha_weights[cid][0,:text_length, :mol_length]\n",
        "\n",
        "  \n",
        "  if (i+1) % 1000 == 0: print(i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKRm1JIGbzE6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "path = \"input/\"\n",
        "with open(path + \"mha_weights.pkl\", 'wb') as fp:\n",
        "  pickle.dump(mha_weights, fp)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Attention_submit.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
