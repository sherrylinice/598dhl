{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GCN_submit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnGSv6cxVKj"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.8.0+cu101.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import math\n",
        "\n",
        "import csv\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import BertTokenizerFast, BertModel\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn1sOS313q5T"
      },
      "source": [
        "import os.path as osp\n",
        "import zipfile\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import download_url, Data\n",
        "from torch_geometric.data import Dataset as GeoDataset\n",
        "from torch_geometric.data import DataLoader as GeoDataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gpao3ymyTBg"
      },
      "source": [
        "#Need a special generator for random sampling:\n",
        "\n",
        "\n",
        "class GenerateData():\n",
        "  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
        "    self.path_train = path_train\n",
        "    self.path_val = path_val\n",
        "    self.path_test = path_test\n",
        "    self.path_molecules = path_molecules\n",
        "    self.path_token_embs = path_token_embs\n",
        "\n",
        "    self.text_trunc_length = 256 \n",
        "\n",
        "    self.prep_text_tokenizer()\n",
        "    \n",
        "    self.load_substructures()\n",
        "\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.store_descriptions()\n",
        "    \n",
        "  def load_substructures(self):\n",
        "    self.molecule_sentences = {}\n",
        "    self.molecule_tokens = {}\n",
        "\n",
        "    total_tokens = set()\n",
        "    self.max_mol_length = 0\n",
        "    with open(self.path_molecules) as f:\n",
        "      for line in f:\n",
        "        spl = line.split(\":\")\n",
        "        cid = spl[0]\n",
        "        tokens = spl[1].strip()\n",
        "        self.molecule_sentences[cid] = tokens\n",
        "        t = tokens.split()\n",
        "        total_tokens.update(t)\n",
        "        size = len(t)\n",
        "        if size > self.max_mol_length: self.max_mol_length = size\n",
        "\n",
        "\n",
        "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
        "\n",
        "\n",
        "\n",
        "  def prep_text_tokenizer(self):\n",
        "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        " \n",
        "\n",
        "  def store_descriptions(self):\n",
        "    self.descriptions = {}\n",
        "    \n",
        "    self.mols = {}\n",
        "\n",
        "\n",
        "\n",
        "    self.training_cids = []\n",
        "    #get training set cids...\n",
        "    with open(self.path_train) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.training_cids.append(line['cid'])\n",
        "        \n",
        "    self.validation_cids = []\n",
        "    #get validation set cids...\n",
        "    with open(self.path_val) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.validation_cids.append(line['cid'])\n",
        "\n",
        "    self.test_cids = []\n",
        "    #get test set cids...\n",
        "    with open(self.path_test) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.test_cids.append(line['cid'])\n",
        "\n",
        "  def generate_examples_train(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.training_cids)\n",
        "\n",
        "    for cid in self.training_cids:\n",
        "      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_input['input_ids'].squeeze(),\n",
        "                'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },\n",
        "          },\n",
        "      }\n",
        "\n",
        "\n",
        "  def generate_examples_val(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.validation_cids)\n",
        "\n",
        "    for cid in self.validation_cids:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                         max_length=self.text_trunc_length, return_tensors = 'np')\n",
        "\n",
        "        mol_input = []\n",
        "\n",
        "        yield {\n",
        "            'cid': cid,\n",
        "            'input': {\n",
        "                'text': {\n",
        "                  'input_ids': text_input['input_ids'].squeeze(),\n",
        "                  'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "                },\n",
        "                'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "  def generate_examples_test(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.test_cids)\n",
        "\n",
        "    for cid in self.test_cids:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                         max_length=self.text_trunc_length, return_tensors = 'np')\n",
        "\n",
        "        mol_input = []\n",
        "\n",
        "        yield {\n",
        "            'cid': cid,\n",
        "            'input': {\n",
        "                'text': {\n",
        "                  'input_ids': text_input['input_ids'].squeeze(),\n",
        "                  'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "                },\n",
        "                'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "mounted_path_token_embs = \"input/token_embedding_dict.npy\"\n",
        "mounted_path_train = \"input/mol2vec_ChEBI_20_training.txt\"\n",
        "mounted_path_val = \"input/mol2vec_ChEBI_20_val.txt\"\n",
        "mounted_path_test = \"input/mol2vec_ChEBI_20_test.txt\"\n",
        "mounted_path_molecules = \"input/ChEBI_defintions_substructure_corpus.cp\"\n",
        "gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zck-zGTa8JOv"
      },
      "source": [
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, gen, length):\n",
        "      'Initialization'\n",
        "\n",
        "      self.gen = gen\n",
        "      self.it = iter(self.gen())\n",
        "\n",
        "      self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "      'Denotes the total number of samples'\n",
        "      return self.length\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      'Generates one sample of data'\n",
        "\n",
        "      try:\n",
        "        ex = next(self.it)\n",
        "      except StopIteration:\n",
        "        self.it = iter(self.gen())\n",
        "        ex = next(self.it)\n",
        "\n",
        "      X = ex['input']\n",
        "      y = 1\n",
        "\n",
        "      return X, y\n",
        "\n",
        "training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n",
        "validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n",
        "test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fj8h8vhk3W0"
      },
      "source": [
        "\n",
        "# Parameters\n",
        "params = {'batch_size': gt.batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "training_generator = DataLoader(training_set, **params)\n",
        "validation_generator = DataLoader(validation_set, **params)\n",
        "test_generator = DataLoader(test_set, **params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q_Y5pNF3uxO"
      },
      "source": [
        "\n",
        "\n",
        "class MoleculeGraphDataset(GeoDataset):\n",
        "    def __init__(self, root, cids, data_path, gt, transform=None, pre_transform=None):\n",
        "        self.cids = cids\n",
        "        self.data_path = data_path\n",
        "        self.gt = gt\n",
        "        super(MoleculeGraphDataset, self).__init__(root, transform, pre_transform)\n",
        "        \n",
        "        self.idx_to_cid = {}\n",
        "        i = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            cid = int(raw_path.split('/')[-1][:-6])\n",
        "            self.idx_to_cid[i] = cid\n",
        "            i += 1\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [cid + \".graph\" for cid in self.cids]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data_{}.pt'.format(cid) for cid in self.cids]\n",
        "\n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        shutil.copy(self.data_path, os.path.join(self.raw_dir, \"/mol_graphs.zip\"))\n",
        "        \n",
        "    def process_graph(self, raw_path):\n",
        "      edge_index  = []\n",
        "      x = []\n",
        "      with open(raw_path, 'r') as f:\n",
        "        next(f)\n",
        "        for line in f: #edges\n",
        "          if line != \"\\n\":\n",
        "            edge = *map(int, line.split()), \n",
        "            edge_index.append(edge)\n",
        "          else:\n",
        "            break\n",
        "        next(f)\n",
        "        for line in f: #get mol2vec features:\n",
        "          substruct_id = line.strip().split()[-1]\n",
        "          if substruct_id in self.gt.token_embs:\n",
        "            x.append(self.gt.token_embs[substruct_id])\n",
        "          else:\n",
        "            x.append(self.gt.token_embs['UNK'])\n",
        "\n",
        "        return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self):\n",
        "      \n",
        "        with zipfile.ZipFile(os.path.join(self.raw_dir, \"/mol_graphs.zip\"), 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.raw_dir)\n",
        "\n",
        "\n",
        "        i = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            # Read data from `raw_path`.\n",
        "\n",
        "            cid = int(raw_path.split('/')[-1][:-6])\n",
        "\n",
        "            edge_index, x = self.process_graph(raw_path)\n",
        "            data = Data(x=x, edge_index = edge_index)\n",
        "\n",
        "            if self.pre_filter is not None and not self.pre_filter(data):\n",
        "                continue\n",
        "\n",
        "            if self.pre_transform is not None:\n",
        "                data = self.pre_transform(data)\n",
        "\n",
        "            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
        "            i += 1\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(self.idx_to_cid[idx])))\n",
        "        return data\n",
        "\n",
        "    def get_cid(self, cid):\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
        "        return data\n",
        "\n",
        "#To get specific lists...\n",
        "\n",
        "class CustomGraphCollater(object):\n",
        "    def __init__(self, dataset, follow_batch = [], exclude_keys = []):\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def collate(self, batch):\n",
        "        elem = batch[0]\n",
        "        if isinstance(elem, Data):\n",
        "            return Batch.from_data_list(batch) \n",
        "            \n",
        "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
        "\n",
        "    def __call__(self, cids):\n",
        "      \n",
        "        return self.collate([self.dataset.get_cid(int(cid)) for cid in cids])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEEojof83vRE"
      },
      "source": [
        "root = 'graph-data/'\n",
        "graph_data_path = \"input/mol_graphs.zip\"\n",
        "\n",
        "\n",
        "mg_data_tr = MoleculeGraphDataset(root, gt.training_cids, graph_data_path, gt)\n",
        "graph_batcher_tr = CustomGraphCollater(mg_data_tr)\n",
        "\n",
        "mg_data_val = MoleculeGraphDataset(root, gt.validation_cids, graph_data_path, gt)\n",
        "graph_batcher_val = CustomGraphCollater(mg_data_val)\n",
        "\n",
        "mg_data_test = MoleculeGraphDataset(root, gt.test_cids, graph_data_path, gt)\n",
        "graph_batcher_test = CustomGraphCollater(mg_data_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aksj743St9ga"
      },
      "source": [
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, graph_hidden_channels, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "\n",
        "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "        \n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "        self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm((nout))\n",
        "        self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "        \n",
        "        #For GCN:\n",
        "        self.conv1 = GCNConv(mg_data_val.num_node_features, graph_hidden_channels)\n",
        "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
        "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
        "\n",
        "\n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "        \n",
        "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n",
        "      \n",
        "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
        "\n",
        "        text_x = text_encoder_output['pooler_output']\n",
        "        text_x = self.text_hidden1(text_x)\n",
        "\n",
        "\n",
        "        #Obtain node embeddings \n",
        "        x = graph_batch.x\n",
        "        edge_index = graph_batch.edge_index\n",
        "        batch = graph_batch.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, graph_hidden_channels]\n",
        "\n",
        "        \n",
        "        x = self.mol_hidden1(x).relu()\n",
        "        x = self.mol_hidden2(x).relu()\n",
        "        x = self.mol_hidden3(x)\n",
        "\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        text_x = self.ln2(text_x)\n",
        "\n",
        "        x = x * torch.exp(self.temp)\n",
        "        text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGMF8AZcB2Zy"
      },
      "source": [
        "model = Model(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nhid = 600, nout = 300, graph_hidden_channels = 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "source": [
        "import torch.optim as optim\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "init_lr = 1e-4 \n",
        "bert_lr = 3e-5\n",
        "bert_params = list(model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(training_generator) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1heECu1nVRB"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "tmp = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HytSaAyHNBuZ"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_func(v1, v2):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  labels = torch.arange(logits.shape[0]).to(device)\n",
        "  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1), labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtfDFAnN_Neu"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "mounted_path = \"GCN_output/\"\n",
        "if not os.path.exists(mounted_path):\n",
        "  os.makedirs(mounted_path)\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    \n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "    for i, d in enumerate(training_generator):\n",
        "        batch, labels = d\n",
        "        # Transfer to GPU\n",
        "        \n",
        "        text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "        text = batch['text']['input_ids'].to(device)\n",
        "        text_mask = text_mask.to(device)\n",
        "        graph_batch = graph_batcher_tr(d[0]['molecule']['cid']).to(device)\n",
        "\n",
        "\n",
        "        text_out, chem_out = model(text, graph_batch, text_mask)\n",
        "        \n",
        "        loss = loss_func(text_out, chem_out).to(device)\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "    train_losses.append(running_loss / (i+1))\n",
        "    train_acc.append(running_acc / (i+1))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "      start_time = time.time()\n",
        "      running_acc = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, d in enumerate(validation_generator):\n",
        "          batch, labels = d\n",
        "          # Transfer to GPU\n",
        "        \n",
        "          text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "          text = batch['text']['input_ids'].to(device)\n",
        "          text_mask = text_mask.to(device)\n",
        "          graph_batch = graph_batcher_val(d[0]['molecule']['cid']).to(device)\n",
        "\n",
        "\n",
        "          text_out, chem_out = model(text, graph_batch, text_mask)\n",
        "        \n",
        "          loss = loss_func(text_out, chem_out).to(device)\n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "      val_losses.append(running_loss / (i+1))\n",
        "      val_acc.append(running_acc / (i+1))\n",
        "\n",
        "\n",
        "      min_loss = np.min(val_losses)\n",
        "      if val_losses[-1] == min_loss:\n",
        "          torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n",
        "      \n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "\n",
        "#Save last accuracy: \n",
        "torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzSbNJ4fWGN6"
      },
      "source": [
        "## Extract Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCP_DuaaWKtV"
      },
      "source": [
        "\n",
        "mounted_path = \"GCN_output/\"\n",
        "\n",
        "cids_train = np.array([])\n",
        "cids_val = np.array([])\n",
        "cids_test = np.array([])\n",
        "chem_embeddings_train = np.array([])\n",
        "text_embeddings_train = np.array([])\n",
        "chem_embeddings_val = np.array([])\n",
        "text_embeddings_val = np.array([])\n",
        "chem_embeddings_test = np.array([])\n",
        "text_embeddings_test = np.array([])\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, d in enumerate(gt.generate_examples_train()):\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    graph_batch = graph_batcher_tr([d['input']['molecule']['cid']]).to(device)\n",
        "    graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
        "    text_emb, chem_emb = model(text, graph_batch, text_mask)\n",
        "    \n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_train = np.concatenate((cids_train, cid)) if cids_train.size else cid\n",
        "    chem_embeddings_train = np.concatenate((chem_embeddings_train, chem_emb)) if chem_embeddings_train.size else chem_emb\n",
        "    text_embeddings_train = np.concatenate((text_embeddings_train, text_emb)) if text_embeddings_train.size else text_emb\n",
        "\n",
        "    if (i+1) % 1000 == 0: print(i+1, \"samples eval.\")\n",
        "\n",
        "    \n",
        "  print(cids_train.shape, chem_embeddings_train.shape)\n",
        "\n",
        "  for d in gt.generate_examples_val():\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    graph_batch = graph_batcher_val([d['input']['molecule']['cid']]).to(device)\n",
        "    graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
        "\n",
        "    text_emb, chem_emb = model(text, graph_batch, text_mask)\n",
        "    \n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_val = np.concatenate((cids_val, cid)) if cids_val.size else cid\n",
        "    chem_embeddings_val = np.concatenate((chem_embeddings_val, chem_emb)) if chem_embeddings_val.size else chem_emb\n",
        "    text_embeddings_val = np.concatenate((text_embeddings_val, text_emb)) if text_embeddings_val.size else text_emb\n",
        "\n",
        "  print(cids_val.shape, chem_embeddings_val.shape)\n",
        "\n",
        "  for d in gt.generate_examples_test():\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    graph_batch = graph_batcher_test([d['input']['molecule']['cid']]).to(device)\n",
        "    graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
        "\n",
        "    text_emb, chem_emb = model(text, graph_batch, text_mask)\n",
        "    \n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n",
        "    chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n",
        "    text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n",
        "\n",
        "print(cids_test.shape, chem_embeddings_test.shape)\n",
        "\n",
        "emb_path = mounted_path+\"embeddings/\"\n",
        "if not os.path.exists(emb_path):\n",
        "  os.mkdir(emb_path)\n",
        "np.save(emb_path+\"cids_train.npy\", cids_train)\n",
        "np.save(emb_path+\"cids_val.npy\", cids_val)\n",
        "np.save(emb_path+\"cids_test.npy\", cids_test)\n",
        "np.save(emb_path+\"chem_embeddings_train.npy\", chem_embeddings_train)\n",
        "np.save(emb_path+\"chem_embeddings_val.npy\", chem_embeddings_val)\n",
        "np.save(emb_path+\"chem_embeddings_test.npy\", chem_embeddings_test)\n",
        "np.save(emb_path+\"text_embeddings_train.npy\", text_embeddings_train)\n",
        "np.save(emb_path+\"text_embeddings_val.npy\", text_embeddings_val)\n",
        "np.save(emb_path+\"text_embeddings_test.npy\", text_embeddings_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}