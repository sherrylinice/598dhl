{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_submit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnGSv6cxVKj"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gpao3ymyTBg"
      },
      "source": [
        "#Need a special generator for random sampling:\n",
        "\n",
        "class GenerateData():\n",
        "  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
        "    self.path_train = path_train\n",
        "    self.path_val = path_val\n",
        "    self.path_test = path_test\n",
        "    self.path_molecules = path_molecules\n",
        "    self.path_token_embs = path_token_embs\n",
        "\n",
        "    self.text_trunc_length = 256 \n",
        "\n",
        "    self.prep_text_tokenizer()\n",
        "    \n",
        "    self.load_substructures()\n",
        "\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.store_descriptions()\n",
        "    \n",
        "  def load_substructures(self):\n",
        "    self.molecule_sentences = {}\n",
        "    self.molecule_tokens = {}\n",
        "\n",
        "    total_tokens = set()\n",
        "    self.max_mol_length = 0\n",
        "    with open(self.path_molecules) as f:\n",
        "      for line in f:\n",
        "        spl = line.split(\":\")\n",
        "        cid = spl[0]\n",
        "        tokens = spl[1].strip()\n",
        "        self.molecule_sentences[cid] = tokens\n",
        "        t = tokens.split()\n",
        "        total_tokens.update(t)\n",
        "        size = len(t)\n",
        "        if size > self.max_mol_length: self.max_mol_length = size\n",
        "\n",
        "\n",
        "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
        "\n",
        "\n",
        "\n",
        "  def prep_text_tokenizer(self):\n",
        "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        " \n",
        "\n",
        "  def store_descriptions(self):\n",
        "    self.descriptions = {}\n",
        "    \n",
        "    self.mols = {}\n",
        "\n",
        "\n",
        "\n",
        "    self.training_cids = []\n",
        "    #get training set cids...\n",
        "    with open(self.path_train) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.training_cids.append(line['cid'])\n",
        "        \n",
        "    self.validation_cids = []\n",
        "    #get validation set cids...\n",
        "    with open(self.path_val) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.validation_cids.append(line['cid'])\n",
        "\n",
        "    self.test_cids = []\n",
        "    #get test set cids...\n",
        "    with open(self.path_test) as f:\n",
        "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
        "      for n, line in enumerate(reader):\n",
        "        self.descriptions[line['cid']] = line['desc']\n",
        "        self.mols[line['cid']] = line['mol2vec']\n",
        "        self.test_cids.append(line['cid'])\n",
        "\n",
        "  def generate_examples_train(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.training_cids)\n",
        "\n",
        "    for cid in self.training_cids:\n",
        "      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_input['input_ids'].squeeze(),\n",
        "                'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },\n",
        "          },\n",
        "      }\n",
        "\n",
        "\n",
        "  def generate_examples_val(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.validation_cids)\n",
        "\n",
        "    for cid in self.validation_cids:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                         max_length=self.text_trunc_length, return_tensors = 'np')\n",
        "\n",
        "        mol_input = []\n",
        "\n",
        "        yield {\n",
        "            'cid': cid,\n",
        "            'input': {\n",
        "                'text': {\n",
        "                  'input_ids': text_input['input_ids'].squeeze(),\n",
        "                  'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "                },\n",
        "                'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "  def generate_examples_test(self):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(self.test_cids)\n",
        "\n",
        "    for cid in self.test_cids:\n",
        "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length', \n",
        "                                         max_length=self.text_trunc_length, return_tensors = 'np')\n",
        "\n",
        "        mol_input = []\n",
        "\n",
        "        yield {\n",
        "            'cid': cid,\n",
        "            'input': {\n",
        "                'text': {\n",
        "                  'input_ids': text_input['input_ids'].squeeze(),\n",
        "                  'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "                },\n",
        "                'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "                }\n",
        "            },\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "mounted_path_token_embs = \"input/token_embedding_dict.npy\"\n",
        "mounted_path_train = \"input/mol2vec_ChEBI_20_training.txt\"\n",
        "mounted_path_val = \"input/mol2vec_ChEBI_20_val.txt\"\n",
        "mounted_path_test = \"input/mol2vec_ChEBI_20_test.txt\"\n",
        "mounted_path_molecules = \"input/ChEBI_defintions_substructure_corpus.cp\"\n",
        "gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zck-zGTa8JOv"
      },
      "source": [
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, gen, length):\n",
        "      'Initialization'\n",
        "\n",
        "      self.gen = gen\n",
        "      self.it = iter(self.gen())\n",
        "\n",
        "      self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "      'Denotes the total number of samples'\n",
        "      return self.length\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      'Generates one sample of data'\n",
        "\n",
        "      try:\n",
        "        ex = next(self.it)\n",
        "      except StopIteration:\n",
        "        self.it = iter(self.gen())\n",
        "        ex = next(self.it)\n",
        "\n",
        "      X = ex['input']\n",
        "      y = 1\n",
        "\n",
        "      return X, y\n",
        "\n",
        "training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n",
        "validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n",
        "test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fj8h8vhk3W0"
      },
      "source": [
        "\n",
        "# Parameters\n",
        "params = {'batch_size': gt.batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "training_generator = DataLoader(training_set, **params)\n",
        "validation_generator = DataLoader(validation_set, **params)\n",
        "test_generator = DataLoader(test_set, **params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aksj743St9ga"
      },
      "source": [
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "\n",
        "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "        \n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.mol_hidden1 = nn.Linear(nout, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
        "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
        "        \n",
        "\n",
        "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "        self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm((nout))\n",
        "        self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "        \n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "        \n",
        "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "    def forward(self, text, molecule, text_mask = None, molecule_mask = None):\n",
        "      \n",
        "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
        "\n",
        "        text_x = text_encoder_output['pooler_output']\n",
        "        text_x = self.text_hidden1(text_x)\n",
        "\n",
        "        x = self.relu(self.mol_hidden1(molecule))\n",
        "        x = self.relu(self.mol_hidden2(x))\n",
        "        x = self.mol_hidden3(x)\n",
        "\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        text_x = self.ln2(text_x)\n",
        "\n",
        "        x = x * torch.exp(self.temp)\n",
        "        text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGMF8AZcB2Zy"
      },
      "source": [
        "model = Model(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nhid = 600, nout = 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "source": [
        "import torch.optim as optim\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "init_lr = 1e-4 \n",
        "bert_lr = 3e-5\n",
        "bert_params = list(model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(training_generator) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1heECu1nVRB"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "tmp = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HytSaAyHNBuZ"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_func(v1, v2):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  labels = torch.arange(logits.shape[0]).to(device)\n",
        "  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1), labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtfDFAnN_Neu"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "mounted_path = \"MLP_outputs/\"\n",
        "if not os.path.exists(mounted_path):\n",
        "  os.mkdir(mounted_path)\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    \n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "    for i, d in enumerate(training_generator):\n",
        "        batch, labels = d\n",
        "        # Transfer to GPU\n",
        "        \n",
        "        text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "        text = batch['text']['input_ids'].to(device)\n",
        "        text_mask = text_mask.to(device)\n",
        "        molecule = batch['molecule']['mol2vec'].float().to(device)\n",
        "\n",
        "        text_out, chem_out = model(text, molecule, text_mask)\n",
        "        \n",
        "        loss = loss_func(text_out, chem_out).to(device)\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "    train_losses.append(running_loss / (i+1))\n",
        "    train_acc.append(running_acc / (i+1))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "      start_time = time.time()\n",
        "      running_acc = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, d in enumerate(validation_generator):\n",
        "          batch, labels = d\n",
        "          # Transfer to GPU\n",
        "        \n",
        "          text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "          text = batch['text']['input_ids'].to(device)\n",
        "          text_mask = text_mask.to(device)\n",
        "          molecule = batch['molecule']['mol2vec'].float().to(device)\n",
        "\n",
        "          \n",
        "\n",
        "          text_out, chem_out = model(text, molecule, text_mask)\n",
        "        \n",
        "          loss = loss_func(text_out, chem_out).to(device)\n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "      val_losses.append(running_loss / (i+1))\n",
        "      val_acc.append(running_acc / (i+1))\n",
        "\n",
        "\n",
        "      min_loss = np.min(val_losses)\n",
        "      if val_losses[-1] == min_loss:\n",
        "          torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n",
        "      \n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzSbNJ4fWGN6"
      },
      "source": [
        "## Extract Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCP_DuaaWKtV"
      },
      "source": [
        "\n",
        "mounted_path = \"MLP_outputs/\"\n",
        "\n",
        "cids_train = np.array([])\n",
        "cids_val = np.array([])\n",
        "cids_test = np.array([])\n",
        "chem_embeddings_train = np.array([])\n",
        "text_embeddings_train = np.array([])\n",
        "chem_embeddings_val = np.array([])\n",
        "text_embeddings_val = np.array([])\n",
        "chem_embeddings_test = np.array([])\n",
        "text_embeddings_test = np.array([])\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, d in enumerate(gt.generate_examples_train()):\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
        "    text_emb, chem_emb = model(text, molecule, text_mask)\n",
        "    \n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_train = np.concatenate((cids_train, cid)) if cids_train.size else cid\n",
        "    chem_embeddings_train = np.concatenate((chem_embeddings_train, chem_emb)) if chem_embeddings_train.size else chem_emb\n",
        "    text_embeddings_train = np.concatenate((text_embeddings_train, text_emb)) if text_embeddings_train.size else text_emb\n",
        "\n",
        "    if (i+1) % 100 == 0: print(i+1, \"samples eval.\")\n",
        "\n",
        "    \n",
        "  print(cids_train.shape, chem_embeddings_train.shape)\n",
        "\n",
        "  for d in gt.generate_examples_val():\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
        "    text_emb, chem_emb = model(text, molecule, text_mask)\n",
        "    \n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_val = np.concatenate((cids_val, cid)) if cids_val.size else cid\n",
        "    chem_embeddings_val = np.concatenate((chem_embeddings_val, chem_emb)) if chem_embeddings_val.size else chem_emb\n",
        "    text_embeddings_val = np.concatenate((text_embeddings_val, text_emb)) if text_embeddings_val.size else text_emb\n",
        "\n",
        "  print(cids_val.shape, chem_embeddings_val.shape)\n",
        "  \n",
        "  for d in gt.generate_examples_test():\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
        "    text_emb, chem_emb = model(text, molecule, text_mask)\n",
        "    \n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n",
        "    chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n",
        "    text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n",
        "\n",
        "print(cids_test.shape, chem_embeddings_test.shape)\n",
        "\n",
        "emb_path = mounted_path+\"embeddings/\"\n",
        "if not os.path.exists(emb_path):\n",
        "  os.mkdir(emb_path)\n",
        "np.save(emb_path+\"cids_train.npy\", cids_train)\n",
        "np.save(emb_path+\"cids_val.npy\", cids_val)\n",
        "np.save(emb_path+\"cids_test.npy\", cids_test)\n",
        "np.save(emb_path+\"chem_embeddings_train.npy\", chem_embeddings_train)\n",
        "np.save(emb_path+\"chem_embeddings_val.npy\", chem_embeddings_val)\n",
        "np.save(emb_path+\"chem_embeddings_test.npy\", chem_embeddings_test)\n",
        "np.save(emb_path+\"text_embeddings_train.npy\", text_embeddings_train)\n",
        "np.save(emb_path+\"text_embeddings_val.npy\", text_embeddings_val)\n",
        "np.save(emb_path+\"text_embeddings_test.npy\", text_embeddings_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}